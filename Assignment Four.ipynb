{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Course (980)\n",
    "## Assignment Four \n",
    "\n",
    "__Assignment Goals__:\n",
    "\n",
    "- Implementing Fully Connected AutoEncoders\n",
    "- Implementing Convolutional AutoEncoders\n",
    "- Understand Variational Autoncoder intuition\n",
    "\n",
    "\n",
    "In this assignment, you will be asked to design a Fully Connected and a CNN AutoEncoder. With a simple change in your Fully Connected AutoEncoder, you will become more familiar with Variational AutoEncoder. \n",
    "\n",
    "__DataSet:__ In this Assignment, you will use the MNIST handwritten digit database. You can use  (x_train, _), (x_test, _)  = tensorflow.keras.datasets.mnist.load_data() to load the dataset.\n",
    "\n",
    "1. (30 points) Implement a Fully Connected AutoEncoder in TensorFlow (cf. Chapter 7). Your AutoEncoder should have a bottleneck with two neurons and Mean Squared Error (MSE) as the objective function. In an AutoEncoder, the layer with the least number of neurons is referred to as a bottleneck. Train your model on MNIST. Plot the train and test loss. Randomly select 10 images from the test set, encode them and visualize the decoded images.\n",
    "     \n",
    "2. (35 points) Implement a convolutional AutoEncoder (CAE) that uses only the following types of layers: convolution, pooling, upsampling and transpose. You are limited to use MSE. The encoder and decoder should include one or more layers, with the size and number of filters chosen by you. Start with a bottleneck of size 2, train your model on MNIST and plot the train and test loss. Randomly select 10 images from the test set, encode them and visualize the decoded images. Are the reconstructed images readable for humans? If not, try to find a CAE architecture, including a larger bottleneck, that is powerful enough to generate readable images. The bottleneck should be as small as possible for readability, this is part of the grading criteria.\n",
    "\n",
    "3. (35 points) This question is about using an AutoEncoder to generate similar but not identical hand digits. We use a naive approach: Try to see if a trained decoder can map randomly generated inputs (random numbers) to a recognizable hand-written digit. \n",
    "    1. Start with your Fully Connected and trained AutoEncoder from part 1. Try to generate new images by inputting some random numbers  to the decoder (i.e. the bottleneck layer) and report your results. Hint: This is not easy. You probably want to input at least 10 random numbers. \n",
    "    2. Now restrict the AutoEncoder hidden bottleneck layer(s) to have a standard multi-variate normal distribution with mean zeroes and the identity matrix as variance (i.e. no correlations). Retrain the Fully Connected AutoEncoder with the normalized bottleneck. Now randomly generate inputs to the bottleneck layer that are drawn from the multi-variate standard normal distribution, and use the random inputs to generate new images. Report your result.\n",
    "    3. Are the output images different between 1) and 2)? If so, why do you think this difference occurs?\n",
    "\n",
    "4. (20 points) Optional: change the AutoEncoder which you developed in the last part of section 3 so that it becomes a Variational AutoEncoder (Introduced by Kingma 2014; see Chapter 7.1). Does the VAE produce a different quality of output image?\n",
    "\n",
    "\n",
    "\n",
    "__Submission Notes__:\n",
    "\n",
    "Please use Jupyter Notebook. The notebook should include the final code, results, and answers. You should submit your Notebook in .pdf and .ipynb format. (penalty 10 points).\n",
    "Your AutoEncoders should have only one bottleneck.\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "__Instructions__:\n",
    "\n",
    "The university policy on academic dishonesty and plagiarism (cheating) will be taken very seriously in this course. Everything submitted should be your writing or coding. You must not let other students copy your work. Spelling and grammar count.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from tensorflow.python.client import device_lib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# normalize the data\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               768       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 784)               201488    \n",
      "=================================================================\n",
      "Total params: 403,730\n",
      "Trainable params: 403,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "''' Part 1: Implement a Fully Connected AutoEncoder in TensorFlow\n",
    "'''\n",
    "\n",
    "# flatten the input image data\n",
    "input_size = 784\n",
    "x_train_flatten = x_train.reshape(-1, input_size)\n",
    "x_test_flatten = x_test.reshape(-1, input_size)\n",
    "\n",
    "# build the network with a bottleneck of two neurons\n",
    "autoencoder_fc = Sequential()\n",
    "autoencoder_fc.add(Dense(256, input_shape=(input_size,), activation='relu'))\n",
    "autoencoder_fc.add(Dense(2, activation='relu'))\n",
    "autoencoder_fc.add(Dense(256, input_shape=(input_size,), activation='relu'))\n",
    "autoencoder_fc.add(Dense(input_size, activation='sigmoid'))\n",
    "\n",
    "autoencoder_fc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3517610447304484559\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6700198133\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2315278478698892369\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0403 - acc: 0.0138 - val_loss: 0.0414 - val_acc: 0.0115\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0402 - acc: 0.0132 - val_loss: 0.0413 - val_acc: 0.0127\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0402 - acc: 0.0138 - val_loss: 0.0413 - val_acc: 0.0131\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0401 - acc: 0.0136 - val_loss: 0.0412 - val_acc: 0.0133\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0400 - acc: 0.0136 - val_loss: 0.0411 - val_acc: 0.0127\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0400 - acc: 0.0142 - val_loss: 0.0412 - val_acc: 0.0126\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0399 - acc: 0.0143 - val_loss: 0.0411 - val_acc: 0.0142\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0398 - acc: 0.0140 - val_loss: 0.0410 - val_acc: 0.0149\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0398 - acc: 0.0138 - val_loss: 0.0411 - val_acc: 0.0140\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0397 - acc: 0.0142 - val_loss: 0.0410 - val_acc: 0.0139\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0396 - acc: 0.0138 - val_loss: 0.0409 - val_acc: 0.0132\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0396 - acc: 0.0139 - val_loss: 0.0409 - val_acc: 0.0149\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0395 - acc: 0.0141 - val_loss: 0.0409 - val_acc: 0.0147\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0395 - acc: 0.0141 - val_loss: 0.0408 - val_acc: 0.0147\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0394 - acc: 0.0142 - val_loss: 0.0408 - val_acc: 0.0151\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0394 - acc: 0.0148 - val_loss: 0.0407 - val_acc: 0.0140\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0393 - acc: 0.0143 - val_loss: 0.0408 - val_acc: 0.0140\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0392 - acc: 0.0143 - val_loss: 0.0408 - val_acc: 0.0151\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0392 - acc: 0.0137 - val_loss: 0.0407 - val_acc: 0.0141\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0392 - acc: 0.0144 - val_loss: 0.0406 - val_acc: 0.0126\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0391 - acc: 0.0139 - val_loss: 0.0406 - val_acc: 0.0134\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0391 - acc: 0.0145 - val_loss: 0.0407 - val_acc: 0.0146\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0390 - acc: 0.0143 - val_loss: 0.0405 - val_acc: 0.0139\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0390 - acc: 0.0145 - val_loss: 0.0406 - val_acc: 0.0141\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0389 - acc: 0.0141 - val_loss: 0.0406 - val_acc: 0.0152\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0389 - acc: 0.0143 - val_loss: 0.0405 - val_acc: 0.0142\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0388 - acc: 0.0146 - val_loss: 0.0404 - val_acc: 0.0148\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0388 - acc: 0.0144 - val_loss: 0.0405 - val_acc: 0.0168\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0387 - acc: 0.0142 - val_loss: 0.0405 - val_acc: 0.0140\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0387 - acc: 0.0140 - val_loss: 0.0404 - val_acc: 0.0143\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0387 - acc: 0.0142 - val_loss: 0.0403 - val_acc: 0.0144\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0386 - acc: 0.0146 - val_loss: 0.0405 - val_acc: 0.0118\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0386 - acc: 0.0144 - val_loss: 0.0405 - val_acc: 0.0126\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0386 - acc: 0.0147 - val_loss: 0.0404 - val_acc: 0.0135\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0385 - acc: 0.0140 - val_loss: 0.0404 - val_acc: 0.0132\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0385 - acc: 0.0148 - val_loss: 0.0403 - val_acc: 0.0144\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0385 - acc: 0.0142 - val_loss: 0.0404 - val_acc: 0.0151\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0384 - acc: 0.0141 - val_loss: 0.0403 - val_acc: 0.0151\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0384 - acc: 0.0141 - val_loss: 0.0403 - val_acc: 0.0136\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0384 - acc: 0.0140 - val_loss: 0.0402 - val_acc: 0.0138\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0384 - acc: 0.0149 - val_loss: 0.0403 - val_acc: 0.0116\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0383 - acc: 0.0139 - val_loss: 0.0403 - val_acc: 0.0136\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0383 - acc: 0.0142 - val_loss: 0.0402 - val_acc: 0.0141\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0382 - acc: 0.0152 - val_loss: 0.0402 - val_acc: 0.0132\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0382 - acc: 0.0138 - val_loss: 0.0402 - val_acc: 0.0122\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0382 - acc: 0.0140 - val_loss: 0.0402 - val_acc: 0.0126\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0382 - acc: 0.0145 - val_loss: 0.0403 - val_acc: 0.0149\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0381 - acc: 0.0142 - val_loss: 0.0401 - val_acc: 0.0141\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0381 - acc: 0.0143 - val_loss: 0.0400 - val_acc: 0.0136\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0381 - acc: 0.0134 - val_loss: 0.0401 - val_acc: 0.0145\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "autoencoder_fc.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "best_model_checkpoint = ModelCheckpoint(\n",
    "    './best_model_fc.pth', \n",
    "    monitor=\"val_acc\", \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False\n",
    ")\n",
    "autoencoder_fc_history = autoencoder_fc.fit(\n",
    "    x_train_flatten, \n",
    "    x_train_flatten, \n",
    "    epochs=50, \n",
    "    batch_size=256, \n",
    "    shuffle=True, \n",
    "    validation_data=(x_test_flatten, x_test_flatten),\n",
    "    callbacks=[best_model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_num = 10\n",
    "autoencoder_fc_best = load_model('./best_model_fc.pth')\n",
    "plt.figure(figsize=(18, 4))\n",
    "for i in range(img_num):\n",
    "    # randomly pick an image from test dataset\n",
    "    chosen_test_img = x_test_flatten[np.random.randint(x_test_flatten.shape[0])]\n",
    "    img_decoded = autoencoder_fc_best.predict(chosen_test_img.reshape(1, 784))\n",
    "    ax = plt.subplot(2, num_images, i+1)\n",
    "    plt.imshow(chosen_test_img.reshape(28, 28))\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax = plt.subplot(2, num_images, num_images+i+ 1)\n",
    "    plt.imshow(img_decoded.reshape(28, 28))\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
